{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Initialize Mission Workspace<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 1 ### Initialize directory paths\n",
    "\n",
    "import datetime, os, geopandas, shutil, zipfile #py7zr\n",
    "## DEFINE DIRECTORIES\n",
    "productionDirectory = \"C:/WorkSpace/IRmapper/.Alberta_mapping/production_workspace\"\n",
    "gdrive = \"G:/Shared drives/GIS Data Center/Delivered To The Goverment/Alberta_deliverables\"\n",
    "#os.chdir(f'{productionDirectory}')\n",
    "## DEFINE DATES\n",
    "delivery_date = datetime.date.today().strftime('%y%m%d')\n",
    "print(f\"COMPLETED - Map delivery day is {delivery_date}, paths imported: {'productionDirectory' in vars()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 2 ### Check not deleting previous work, setup local directories\n",
    "\n",
    "today_folder = f'{productionDirectory}/{delivery_date}'\n",
    "assert False == os.path.exists(today_folder), \"processing folder for this date already exists, go delete if restarting validation\"\n",
    "os.makedirs(f'{today_folder}/_camera_data_{delivery_date}')\n",
    "os.makedirs(f'{today_folder}/_client_request_data_{delivery_date}')\n",
    "cam_file = f'{today_folder}/_camera_data_{delivery_date}'\n",
    "\n",
    "print(f\"COMPLETED - local directiories built: {os.path.exists(cam_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 3 ### Make Gdrive directory\n",
    "\n",
    "month_numeric = datetime.date.today().strftime('%m')\n",
    "month_literal = datetime.date.today().strftime('%B')\n",
    "day_numeric = datetime.date.today().strftime('%d')\n",
    "gdrive_deliverables_path = f'{gdrive}/{month_numeric}_{month_literal}/{month_literal.upper()}_{day_numeric}/'\n",
    "if not os.path.exists(gdrive_deliverables_path):\n",
    "    os.makedirs(gdrive_deliverables_path)\n",
    "    print(f\"COMPLETED - cloud folder built: {os.path.exists(gdrive_deliverables_path)}\")\n",
    "else:\n",
    "    print(f\"COMPLETED - cloud folder built: False (already existed)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYST ACTION: paste geodata from gov request email to \"_client_request_data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 4 ### Extract zip files from gov request geodata\n",
    "\n",
    "counter = 0\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:\n",
    "        if name.endswith('.zip'):\n",
    "            with zipfile.ZipFile(os.path.join(root, name), 'r') as zip:\n",
    "                zip.extractall(root)\n",
    "                counter += 1\n",
    "                print(f'{counter}. zip file \"{name}\" extracted')\n",
    "        #elif name.endswith('.7z'):\n",
    "        #    with py7zr.SevenZipFile(os.path.join(root, name), 'r') as z:\n",
    "        #        z.extractall(root)\n",
    "        \n",
    "if counter == 0:\n",
    "    print(\"COMPLETED - no compressed files in request data\")\n",
    "else:\n",
    "    print(\"---\")\n",
    "    print(f\"COMPLETED - {counter} compressed file(s) extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK 5 ### Request data validation system\n",
    "\n",
    "## Validation: verify file contains column \"FIRE_NUMBE\"\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:        \n",
    "        if name.endswith('.shp'):\n",
    "            shapefile = geopandas.read_file(os.path.join(root, name))\n",
    "            fire_columns = shapefile.columns\n",
    "            assert \"FIRE_NUMBE\" in fire_columns, f\"Shapefile attribute table doesn't contain field FIRE_NUMBE - file: {name} - build identification field before proceeding\"\n",
    "print(\"All files contain FIRE_NUMBE identification column\")\n",
    "\n",
    "## Validation: verify correct CRS\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:\n",
    "        if name.endswith('.shp'):\n",
    "            shapefile = geopandas.read_file(os.path.join(root, name))\n",
    "            assert shapefile.crs == \"EPSG:3400\", f\"CRS incorrect for this jursidiction - file name: {name} - change to EPSG:3400 before proceeding\"\n",
    "print(\"All files have correct CRS\")\n",
    "\n",
    "## Validation: verify each fire is a single multipart object - not multiple singlepart geometries\n",
    "import collections\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:\n",
    "        if name.endswith('.shp'):\n",
    "            shapefile = geopandas.read_file(os.path.join(root, name))\n",
    "            fireID_list = list(shapefile['FIRE_NUMBE'])\n",
    "            duplicated = [k for k, v in collections.Counter(fireID_list).items() if v > 1]\n",
    "            assert duplicated == [], f\"Fire(s) {duplicated} in file {name} is in multiple singlepart features - merge into multipart before proceeding\"\n",
    "print(\"All fires are built of a single multipart geometry\")\n",
    "\n",
    "## Validation: verify no repeated fires between files\n",
    "requested_fires = []\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:        \n",
    "        if name.endswith('.shp'):\n",
    "            shapefile = geopandas.read_file(os.path.join(root, name))\n",
    "            for fire in shapefile['FIRE_NUMBE']:\n",
    "                requested_fires.append(fire)\n",
    "duplicated = [k for k, v in collections.Counter(requested_fires).items() if v > 1]\n",
    "assert duplicated == [], f\"Fires {duplicated} are duplicated across request files - delete down to one instance of fire before proceeding\"\n",
    "print(\"No fire IDs are repeated across different request files\")\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"COMPLETED - all validation checks were passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK 6 ### Unpack fires polygons from request geodata into individual fires workspaces\n",
    "\n",
    "requested_fires = []\n",
    "for root, dirs, files in os.walk(f'{today_folder}/_client_request_data_{delivery_date}'):\n",
    "    for name in files:        \n",
    "        if name.endswith('.shp'):\n",
    "            gdf_fires = geopandas.read_file(os.path.join(root, name))\n",
    "            fires_list = gdf_fires['FIRE_NUMBE'].unique()\n",
    "            print(f'fires_list from {name}: {fires_list}')\n",
    "            for fire in fires_list:\n",
    "                requested_fires.append(fire)\n",
    "                single_fire_gdf = gdf_fires[gdf_fires['FIRE_NUMBE'] == fire]\n",
    "                os.makedirs(f'{today_folder}/{fire}_{delivery_date}/Scratch')\n",
    "                single_fire_gdf.to_file(f'{today_folder}/{fire}_{delivery_date}/Scratch/{fire}_request_perimeter.shp')    \n",
    "if len(requested_fires) == 0:\n",
    "    def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n",
    "    prRed(f'all fires: {requested_fires}, no fires the found in the request')\n",
    "else:\n",
    "    print(f'all fires: {requested_fires}')\n",
    "    print(\"---\")\n",
    "    print(f\"COMPLETED - {len(requested_fires)} workspace(s) built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOCK 7 ### Build workspace for each requested fire + spare fire\n",
    "\n",
    "#add spare fire workspace\n",
    "if \"ZZZ999\" not in requested_fires:\n",
    "    requested_fires.append(\"ZZZ999\")\n",
    "    os.makedirs(f'{today_folder}/ZZZ999_{delivery_date}/Scratch')\n",
    "    open(f'{today_folder}/ZZZ999_{delivery_date}/Scratch/ZZZ999_request_perimeter.shp', \"w+\")\n",
    "\n",
    "#Make subdirectory for each fire post-production mapping\n",
    "for fire in requested_fires:\n",
    "    src=\"C:/WorkSpace/IRmapper/application_materials_v1.0/python_scripts/AB/AB_mapping.ipynb\"\n",
    "    dst=f\"{today_folder}/{fire}_{delivery_date}/{fire}-processing.ipynb\"\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "    #make a copy of the processing code template for local fire\n",
    "    src=\"C:/WorkSpace/IRmapper/application_materials_v1.0/QGIS_projects/AB_template.qgz\"\n",
    "    dst=f\"{today_folder}/{fire}_{delivery_date}/{fire}_{delivery_date}.qgz\"\n",
    "    shutil.copy(src,dst)\n",
    "    print(f\"Built workspace for {fire}\")\n",
    "print(\"---\")\n",
    "print(\"COMPLETED - move on to individual fire processing scripts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
